{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Aqueducts","text":"<p>Aqueducts is a framework to write and execute ETL data pipelines declaratively. Define your data processing workflows in YAML and execute them locally or remotely with built-in cloud storage support.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Declarative Pipeline Configuration: Define ETL pipelines in YAML, JSON, or TOML</li> <li>Pipeline Execution: Run pipelines locally, remotely, or embedded in your applications</li> <li>Data Source Support: Extract from CSV, JSONL, Parquet files, Delta tables, and ODBC databases</li> <li>Data Destination Support: Load data into local files, object stores, or Delta tables</li> <li>Cloud Storage Support: Built-in support for Local, S3, GCS, and Azure Blob storage</li> <li>Advanced Operations: Upsert/Replace/Append operations with partition support</li> <li>Template System: Parameter substitution with <code>${variable}</code> syntax</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to build your first pipeline? Head to Getting Started for installation and a quick tutorial.</p>"},{"location":"#examples","title":"Examples","text":"<p>Browse real-world pipeline examples in the examples folder on GitHub.</p>"},{"location":"#community","title":"Community","text":"<p>Join our Discord community to get help, share your work, and connect with other Aqueducts users:</p> <p></p>"},{"location":"execution/","title":"Execution","text":"<p>Aqueducts supports both local and remote pipeline execution. Use the CLI for local execution or submit jobs to remote executors running in your infrastructure.</p>"},{"location":"execution/#local-execution","title":"Local Execution","text":"<p>Execute pipelines directly on your local machine using the Aqueducts CLI.</p>"},{"location":"execution/#basic-usage","title":"Basic Usage","text":"<pre><code># Run a pipeline\naqueducts run --file pipeline.yml\n\n# Run with parameters\naqueducts run --file pipeline.yml --param key1=value1 --param key2=value2\n\n# Multiple parameters\naqueducts run --file pipeline.yml --params key1=value1 --params key2=value2\n</code></pre>"},{"location":"execution/#supported-formats","title":"Supported Formats","text":"<p>The CLI supports multiple pipeline definition formats:</p> YAMLJSONTOML <pre><code>aqueducts run --file pipeline.yml\n</code></pre> <pre><code>aqueducts run --file pipeline.json\n</code></pre> <pre><code>aqueducts run --file pipeline.toml\n</code></pre> <p>Format Support</p> <p>JSON and TOML support require appropriate feature flags during CLI installation.</p>"},{"location":"execution/#local-execution-benefits","title":"Local Execution Benefits","text":"<ul> <li>Direct access to local files and databases</li> <li>No network overhead for file-based operations</li> <li>Immediate feedback and debugging</li> <li>Full control over execution environment</li> </ul>"},{"location":"execution/#remote-execution","title":"Remote Execution","text":"<p>Execute pipelines on remote executors deployed within your infrastructure, closer to your data sources.</p>"},{"location":"execution/#architecture-overview","title":"Architecture Overview","text":"<p>Remote execution follows this flow:</p> <ol> <li>CLI Client submits pipeline to Remote Executor</li> <li>Remote Executor processes data from Data Sources</li> <li>Remote Executor writes results to Destinations</li> <li>Remote Executor sends status updates back to CLI Client</li> </ol>"},{"location":"execution/#setting-up-remote-execution","title":"Setting Up Remote Execution","text":"<ol> <li> <p>Deploy an Executor:</p> <pre><code># Start executor with API key\naqueducts-executor --api-key your_secret_key --max-memory 4\n</code></pre> </li> <li> <p>Submit Pipeline from CLI:</p> <pre><code># Execute remotely\naqueducts run --file pipeline.yml \\\n  --executor executor-host:3031 \\\n  --api-key your_secret_key \\\n  --param environment=prod\n</code></pre> </li> </ol>"},{"location":"execution/#remote-execution-benefits","title":"Remote Execution Benefits","text":"<ul> <li>Minimize network transfer by processing data close to sources</li> <li>Scale processing power independently of client machines</li> <li>Secure execution within your infrastructure boundaries</li> <li>Centralized resource management with memory limits</li> </ul>"},{"location":"execution/#managing-remote-executions","title":"Managing Remote Executions","text":"<p>Monitor and control remote pipeline executions:</p> <pre><code># Check executor status\ncurl http://executor-host:3031/api/health\n\n# Cancel a running pipeline\naqueducts cancel --executor executor-host:3031 \\\n  --api-key your_secret_key \\\n  --execution-id abc-123\n</code></pre>"},{"location":"execution/#aqueducts-executor","title":"Aqueducts Executor","text":"<p>The Aqueducts Executor is a deployable application for running pipelines within your infrastructure.</p>"},{"location":"execution/#key-features","title":"Key Features","text":"<ul> <li>Remote Execution: Run data pipelines securely within your infrastructure</li> <li>Memory Management: Configure maximum memory usage with DataFusion's memory pool</li> <li>Real-time Feedback: WebSocket communication for live progress updates</li> <li>Cloud Storage Support: Native S3, GCS, and Azure Blob Storage integration</li> <li>Database Connectivity: ODBC support for various database systems</li> <li>Exclusive Execution: Single-pipeline execution for optimal resource utilization</li> </ul>"},{"location":"execution/#docker-deployment-recommended","title":"Docker Deployment (Recommended)","text":"<p>The Docker image includes ODBC support with PostgreSQL drivers pre-installed:</p> <pre><code># Pull from GitHub Container Registry\ndocker pull ghcr.io/vigimite/aqueducts/aqueducts-executor:latest\n\n# Run with command line arguments\ndocker run -d \\\n  --name aqueducts-executor \\\n  -p 3031:3031 \\\n  ghcr.io/vigimite/aqueducts/aqueducts-executor:latest \\\n  --api-key your_secret_key --max-memory 4\n</code></pre>"},{"location":"execution/#environment-variables","title":"Environment Variables","text":"<p>Configure the executor using environment variables:</p> <pre><code>docker run -d \\\n  --name aqueducts-executor \\\n  -p 3031:3031 \\\n  -e AQUEDUCTS_API_KEY=your_secret_key \\\n  -e AQUEDUCTS_HOST=0.0.0.0 \\\n  -e AQUEDUCTS_PORT=3031 \\\n  -e AQUEDUCTS_MAX_MEMORY=4 \\\n  -e AQUEDUCTS_LOG_LEVEL=info \\\n  ghcr.io/vigimite/aqueducts/aqueducts-executor:latest\n</code></pre>"},{"location":"execution/#configuration-options","title":"Configuration Options","text":"Option Description Default Environment Variable <code>--api-key</code> API key for authentication - <code>AQUEDUCTS_API_KEY</code> <code>--host</code> Host address to bind to 0.0.0.0 <code>AQUEDUCTS_HOST</code> <code>--port</code> Port to listen on 8080 <code>AQUEDUCTS_PORT</code> <code>--max-memory</code> Maximum memory usage in GB (0 for unlimited) 0 <code>AQUEDUCTS_MAX_MEMORY</code> <code>--server-url</code> URL of Aqueducts server for registration (optional) - <code>AQUEDUCTS_SERVER_URL</code> <code>--executor-id</code> Unique identifier for this executor auto-generated <code>AQUEDUCTS_EXECUTOR_ID</code> <code>--log-level</code> Logging level (info, debug, trace) info <code>AQUEDUCTS_LOG_LEVEL</code>"},{"location":"execution/#docker-compose-setup","title":"Docker Compose Setup","text":"<p>For local development and testing:</p> <pre><code># Start database only (default)\ndocker-compose up\n\n# Start database + executor\ndocker-compose --profile executor up\n\n# Build and start from source\ndocker-compose --profile executor up --build\n</code></pre> <p>The executor will be available at:</p> <ul> <li>API: <code>http://localhost:3031</code></li> <li>Health check: <code>http://localhost:3031/api/health</code></li> <li>WebSocket: <code>ws://localhost:3031/ws/connect</code></li> </ul>"},{"location":"execution/#manual-installation","title":"Manual Installation","text":"<p>Install using Cargo for custom deployments:</p> <pre><code># Standard installation with cloud storage features\ncargo install aqueducts-executor\n\n# Installation with ODBC support\ncargo install aqueducts-executor --features odbc\n</code></pre>"},{"location":"execution/#api-endpoints","title":"API Endpoints","text":"Endpoint Method Auth Description <code>/api/health</code> GET No Basic health check <code>/ws/connect</code> GET Yes WebSocket endpoint for bidirectional communication"},{"location":"execution/#odbc-configuration","title":"ODBC Configuration","text":"<p>For database connectivity, ODBC support requires the <code>odbc</code> feature flag during installation and proper system configuration.</p>"},{"location":"execution/#installation-requirements","title":"Installation Requirements","text":"<p>First, install Aqueducts with ODBC support:</p> <pre><code># CLI with ODBC support\ncargo install aqueducts-cli --features odbc\n\n# Executor with ODBC support  \ncargo install aqueducts-executor --features odbc\n</code></pre>"},{"location":"execution/#system-dependencies","title":"System Dependencies","text":"Ubuntu/DebianFedora/RHEL/CentOSmacOS <pre><code># Install UnixODBC development libraries\nsudo apt-get update\nsudo apt-get install unixodbc-dev\n\n# PostgreSQL driver\nsudo apt-get install odbc-postgresql\n\n# MySQL driver\nsudo apt-get install libmyodbc\n\n# SQL Server driver (optional)\ncurl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/20.04/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list\nsudo apt-get update\nsudo apt-get install msodbcsql17\n</code></pre> <pre><code># Install UnixODBC development libraries\nsudo dnf install unixODBC-devel\n\n# PostgreSQL driver\nsudo dnf install postgresql-odbc\n\n# MySQL driver\nsudo dnf install mysql-connector-odbc\n\n# SQL Server driver (optional)\nsudo curl -o /etc/yum.repos.d/msprod.repo https://packages.microsoft.com/config/rhel/8/prod.repo\nsudo dnf install msodbcsql17\n</code></pre> <pre><code># Install UnixODBC via Homebrew\nbrew install unixodbc\n\n# PostgreSQL driver\nbrew install psqlodbc\n\n# MySQL driver\nbrew install mysql-connector-c++\n\n# SQL Server driver (optional)\nbrew tap microsoft/mssql-release https://github.com/Microsoft/homebrew-mssql-release\nbrew install msodbcsql17\n</code></pre>"},{"location":"execution/#driver-configuration","title":"Driver Configuration","text":"PostgreSQLMySQL <p>Edit <code>/etc/odbcinst.ini</code> (system) or <code>~/.odbcinst.ini</code> (user):</p> <p>Linux: <pre><code>[PostgreSQL]\nDescription=PostgreSQL ODBC driver\nDriver=/usr/lib/x86_64-linux-gnu/odbc/psqlodbcw.so\nSetup=/usr/lib/x86_64-linux-gnu/odbc/libodbcpsqlS.so\nFileUsage=1\n</code></pre></p> <p>macOS: <pre><code>[PostgreSQL]\nDescription=PostgreSQL ODBC driver\nDriver=/opt/homebrew/lib/psqlodbcw.so\nSetup=/opt/homebrew/lib/libodbcpsqlS.so\nFileUsage=1\n</code></pre></p> <p>Edit <code>/etc/odbcinst.ini</code> (system) or <code>~/.odbcinst.ini</code> (user):</p> <p>Linux: <pre><code>[MySQL]\nDescription=MySQL ODBC driver\nDriver=/usr/lib/x86_64-linux-gnu/odbc/libmyodbc8w.so\nFileUsage=1\n</code></pre></p> <p>macOS: <pre><code>[MySQL]\nDescription=MySQL ODBC driver\nDriver=/opt/homebrew/lib/libmyodbc8w.so\nFileUsage=1\n</code></pre></p>"},{"location":"execution/#data-source-configuration","title":"Data Source Configuration","text":"<p>Configure database connections in <code>/etc/odbc.ini</code> (system) or <code>~/.odbc.ini</code> (user):</p> PostgreSQL DSNMySQL DSN <pre><code>[PostgreSQL-Local]\nDescription=Local PostgreSQL Database\nDriver=PostgreSQL\nServer=localhost\nPort=5432\nDatabase=mydb\nUserName=myuser\nPassword=mypass\nReadOnly=no\nServerType=postgres\nConnSettings=\n</code></pre> <pre><code>[MySQL-Local]\nDescription=Local MySQL Database  \nDriver=MySQL\nServer=localhost\nPort=3306\nDatabase=mydb\nUser=myuser\nPassword=mypass\n</code></pre>"},{"location":"execution/#connection-string-examples","title":"Connection String Examples","text":"PostgreSQLMySQL <pre><code># Using DSN\nsources:\n  - type: odbc\n    name: postgres_data\n    connection_string: \"DSN=PostgreSQL-Local\"\n    load_query: \"SELECT * FROM users WHERE created_at &gt; '2024-01-01'\"\n\n# Direct connection string\nsources:\n  - type: odbc\n    name: postgres_data\n    connection_string: \"Driver={PostgreSQL};Server=localhost;Database=mydb;UID=user;PWD=pass;\"\n    load_query: \"SELECT * FROM users LIMIT 1000\"\n</code></pre> <pre><code># Using DSN\nsources:\n  - type: odbc\n    name: mysql_data\n    connection_string: \"DSN=MySQL-Local\"\n    load_query: \"SELECT * FROM products WHERE price &gt; 100\"\n\n# Direct connection string\nsources:\n  - type: odbc\n    name: mysql_data\n    connection_string: \"Driver={MySQL};Server=localhost;Database=mydb;User=user;Password=pass;\"\n    load_query: \"SELECT * FROM orders WHERE date &gt;= '2024-01-01'\"\n</code></pre>"},{"location":"execution/#testing-your-setup","title":"Testing Your Setup","text":""},{"location":"execution/#1-test-odbc-installation","title":"1. Test ODBC Installation","text":"<pre><code># Check installed drivers\nodbcinst -q -d\n\n# Check configured data sources\nodbcinst -q -s\n</code></pre>"},{"location":"execution/#2-test-database-connection","title":"2. Test Database Connection","text":"<pre><code># Test with isql (interactive SQL)\nisql -v PostgreSQL-Local username password\n\n# Test MySQL connection\nisql -v MySQL-Local username password\n</code></pre>"},{"location":"execution/#3-test-with-aqueducts","title":"3. Test with Aqueducts","text":"<p>Create a minimal test pipeline:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/vigimite/aqueducts/main/json_schema/aqueducts.schema.json\n\nversion: \"v2\"\nsources:\n  - type: odbc\n    name: test_connection\n    connection_string: \"DSN=PostgreSQL-Local\"\n    load_query: \"SELECT 1 as test_column\"\n\nstages:\n  - - name: verify\n      query: \"SELECT * FROM test_connection\"\n      show: 1\n</code></pre> <p>Run the test:</p> <pre><code>aqueducts run --file test-odbc.yml\n</code></pre>"},{"location":"execution/#common-driver-paths","title":"Common Driver Paths","text":"LinuxmacOS <pre><code># PostgreSQL\n/usr/lib/x86_64-linux-gnu/odbc/psqlodbcw.so\n\n# MySQL  \n/usr/lib/x86_64-linux-gnu/odbc/libmyodbc8w.so\n\n# Find drivers\nfind /usr -name \"*odbc*.so\" 2&gt;/dev/null\n</code></pre> <pre><code># PostgreSQL\n/opt/homebrew/lib/psqlodbcw.so\n\n# MySQL\n/opt/homebrew/lib/libmyodbc8w.so\n\n# Find drivers  \nfind /opt/homebrew -name \"*odbc*.so\" 2&gt;/dev/null\n</code></pre>"},{"location":"execution/#performance-considerations","title":"Performance Considerations","text":"<p>Optimization Tips</p> <ul> <li>Limit query results: Use <code>LIMIT</code> clauses to avoid memory issues</li> <li>Filter early: Apply <code>WHERE</code> conditions in your <code>load_query</code></li> <li>Use indexes: Ensure your database queries use appropriate indexes</li> <li>Memory management: Set executor <code>--max-memory</code> limits appropriately</li> </ul>"},{"location":"execution/#odbc-troubleshooting","title":"ODBC Troubleshooting","text":""},{"location":"execution/#driver-loading-issues","title":"Driver Loading Issues","text":"<pre><code># Check if drivers are registered\nodbcinst -q -d\n\n# Test driver loading\nldd /path/to/driver.so  # Linux\notool -L /path/to/driver.so  # macOS\n</code></pre>"},{"location":"execution/#connection-issues","title":"Connection Issues","text":"<pre><code># Enable ODBC tracing for debugging\nexport ODBCSYSINI=/tmp\nexport ODBCINSTINI=/etc/odbcinst.ini\nexport ODBCINI=/etc/odbc.ini\n\n# Test with verbose output\nisql -v DSN_NAME username password\n</code></pre>"},{"location":"execution/#common-error-solutions","title":"Common Error Solutions","text":"<ul> <li>Driver not found: Verify driver paths in <code>odbcinst.ini</code></li> <li>DSN not found: Check <code>odbc.ini</code> configuration</li> <li>Permission denied: Ensure ODBC files are readable</li> <li>Library loading: Install missing system dependencies</li> </ul>"},{"location":"execution/#troubleshooting","title":"Troubleshooting","text":""},{"location":"execution/#common-issues","title":"Common Issues","text":"<p>Local Execution:</p> <ul> <li>Pipeline validation errors: Check YAML syntax and schema compliance</li> <li>Missing features: Ensure CLI was compiled with required feature flags</li> <li>File not found: Verify file paths and permissions</li> </ul> <p>Remote Execution:</p> <ul> <li>Connection timeouts: Check network connectivity and firewall rules</li> <li>Authentication failures: Verify API key configuration</li> <li>Executor busy: Only one pipeline runs at a time per executor</li> <li>Memory errors: Increase <code>--max-memory</code> or optimize pipeline queries</li> </ul> <p>ODBC Issues:</p> <ul> <li>Driver not found: Install database-specific ODBC drivers</li> <li>Connection failures: Verify DSN configuration in <code>odbc.ini</code></li> <li>Permission errors: Check database credentials and network access</li> </ul>"},{"location":"execution/#performance-optimization","title":"Performance Optimization","text":"<p>Memory Management</p> <ul> <li>Set appropriate <code>--max-memory</code> limits for executors</li> <li>Break large queries into smaller stages</li> <li>Add filtering early in the pipeline</li> <li>Use partitioning for large datasets</li> </ul> <p>Network Optimization</p> <ul> <li>Deploy executors close to data sources</li> <li>Use cloud storage in the same region as executors</li> <li>Minimize data movement between stages</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you install Aqueducts and run your first pipeline.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#cli","title":"CLI","text":"<p>The Aqueducts CLI is the primary way to execute pipelines locally or submit them to remote executors.</p> Homebrew (macOS/Linux)Shell InstallerDirect DownloadBuild from Source <pre><code># Add the tap and install\nbrew tap vigimite/aqueducts\nbrew install aqueducts-cli\n</code></pre> <pre><code># One-line installer (Linux, macOS, Windows)\ncurl --proto '=https' --tlsv1.2 -LsSf https://github.com/vigimite/aqueducts/releases/latest/download/aqueducts-installer.sh | sh\n</code></pre> <p>Download pre-built binaries from the latest release.</p> <pre><code># Install with default features (s3, gcs, azure, yaml)\ncargo install aqueducts-cli --locked\n\n# Install with ODBC support\ncargo install aqueducts-cli --locked --features odbc\n\n# Install with minimal features\ncargo install aqueducts-cli --locked --no-default-features --features yaml\n</code></pre>"},{"location":"getting-started/#executor-optional","title":"Executor (Optional)","text":"<p>For advanced use cases, you can deploy executors to run pipelines remotely within your infrastructure. Executors are typically deployed using Docker images. See Execution for detailed setup instructions.</p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#run-your-first-pipeline","title":"Run Your First Pipeline","text":"<p>Execute the simple example pipeline:</p> <pre><code>aqueducts run --file examples/aqueduct_pipeline_simple.yml --param year=2024 --param month=jan\n</code></pre> <p>This pipeline:</p> <ol> <li>Reads temperature data from CSV files</li> <li>Aggregates the data by date and location</li> <li>Enriches with location names</li> <li>Outputs to a Parquet file</li> </ol>"},{"location":"getting-started/#example-pipeline-structure","title":"Example Pipeline Structure","text":"<p>Here's what a basic pipeline looks like:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/vigimite/aqueducts/main/json_schema/aqueducts.schema.json\n\nversion: \"v2\"\nsources:\n  # Read temperature readings from CSV\n  - type: file\n    name: temp_readings\n    format:\n      type: csv\n      options: {}\n    location: ./examples/temp_readings_${month}_${year}.csv\n\nstages:\n  # Aggregate temperature data by date and location\n  - - name: aggregated\n      query: &gt;\n          SELECT\n            cast(timestamp as date) date,\n            location_id,\n            round(avg(temperature_c),2) avg_temp_c\n          FROM temp_readings\n          GROUP by 1,2\n          ORDER by 1 asc\n\n# Write results to Parquet file\ndestination:\n  type: file\n  name: results\n  format:\n    type: parquet\n    options: {}\n  location: ./examples/output_${month}_${year}.parquet\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn Pipeline Development: See Writing Pipelines to understand sources, stages, and destinations</li> <li>Explore Execution Options: Check Execution for local and remote execution patterns</li> <li>Browse Examples: View more complex examples in the examples folder</li> <li>Schema Reference: Detailed configuration options in Schema Reference</li> </ul> <p>Parameter Templates</p> <p>Notice the <code>${month}</code> and <code>${year}</code> parameters in the example. Aqueducts supports parameter substitution to make pipelines reusable across different inputs and outputs.</p> <p>Editor Support</p> <p>The <code>yaml-language-server</code> comment at the top enables autocompletion and validation in VS Code, Neovim, and other editors with YAML Language Server support.</p>"},{"location":"pipelines/","title":"Writing Pipelines","text":"<p>Aqueducts pipelines are declarative YAML configurations that define data processing workflows. This guide covers the key concepts and patterns for building effective pipelines.</p>"},{"location":"pipelines/#pipeline-structure","title":"Pipeline Structure","text":"<p>Every Aqueducts pipeline follows this basic structure:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/vigimite/aqueducts/main/json_schema/aqueducts.schema.json\n\nversion: \"v2\"        # Schema version\nsources: [...]       # Where to read data from\nstages: [...]        # How to transform the data  \ndestination: {...}   # Where to write the results (optional)\n</code></pre>"},{"location":"pipelines/#editor-support-validation","title":"Editor Support &amp; Validation","text":"<p>For the best development experience, add the schema directive at the top of your pipeline files:</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/vigimite/aqueducts/main/json_schema/aqueducts.schema.json\n</code></pre> <p>This enables:</p> <ul> <li>Autocompletion for all configuration options</li> <li>Real-time validation with error highlighting  </li> <li>Inline documentation on hover</li> <li>Schema-aware suggestions for valid values</li> </ul> <p>Supported Editors: - VS Code (with YAML extension) - Neovim (with yaml-language-server) - IntelliJ IDEA / PyCharm - Any editor with YAML Language Server support</p>"},{"location":"pipelines/#data-sources","title":"Data Sources","text":"<p>Sources define where your pipeline reads data from. Aqueducts supports multiple source types:</p>"},{"location":"pipelines/#file-sources","title":"File Sources","text":"<p>Read from individual files in various formats:</p> <pre><code>sources:\n  - type: file\n    name: sales_data\n    format:\n      type: csv\n      options:\n        has_header: true\n        delimiter: \",\"\n    location: ./data/sales.csv\n</code></pre> <p>Supported formats: CSV, Parquet, JSON</p>"},{"location":"pipelines/#directory-sources","title":"Directory Sources","text":"<p>Process all files in a directory:</p> <pre><code>sources:\n  - type: directory\n    name: daily_logs\n    format:\n      type: parquet\n      options: {}\n    location: s3://bucket/logs/\n    partition_columns:\n      - [\"date\", \"date32\"]\n</code></pre>"},{"location":"pipelines/#delta-table-sources","title":"Delta Table Sources","text":"<p>Read from Delta Lake tables:</p> <pre><code>sources:\n  - type: delta\n    name: user_events\n    location: s3://datalake/events/\n    # Optional: read specific version or timestamp\n    version: 42\n    # timestamp: \"2024-01-15T10:30:00Z\"\n</code></pre>"},{"location":"pipelines/#odbc-sources","title":"ODBC Sources","text":"<p>Feature Flag Required</p> <p>ODBC support requires the <code>odbc</code> feature flag during installation.</p> <pre><code>sources:\n  - type: odbc\n    name: postgres_table\n    connection_string: \"Driver={PostgreSQL};Server=localhost;Database=test;\"\n    load_query: \"SELECT * FROM users WHERE created_at &gt; '2024-01-01'\"\n</code></pre>"},{"location":"pipelines/#cloud-storage-configuration","title":"Cloud Storage Configuration","text":"<p>All file and directory sources support cloud storage with authentication:</p> S3Google Cloud StorageAzure Blob Storage <pre><code>sources:\n  - type: file\n    name: s3_data\n    location: s3://my-bucket/data.csv\n    storage_config:\n      AWS_REGION: us-east-1\n      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}\n      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}\n</code></pre> <pre><code>sources:\n  - type: file\n    name: gcs_data\n    location: gs://my-bucket/data.csv\n    storage_config:\n      GOOGLE_SERVICE_ACCOUNT_PATH: /path/to/service-account.json\n</code></pre> <pre><code>sources:\n  - type: file\n    name: azure_data\n    location: abfss://container@account.dfs.core.windows.net/data.csv\n    storage_config:\n      AZURE_STORAGE_ACCOUNT_NAME: myaccount\n      AZURE_STORAGE_ACCOUNT_KEY: ${AZURE_KEY}\n</code></pre>"},{"location":"pipelines/#data-transformation-stages","title":"Data Transformation (Stages)","text":"<p>Stages define SQL queries that transform your data. Aqueducts uses DataFusion for SQL processing.</p>"},{"location":"pipelines/#basic-stages","title":"Basic Stages","text":"<pre><code>stages:\n  - - name: daily_summary\n      query: &gt;\n        SELECT \n          date_trunc('day', timestamp) as date,\n          count(*) as events,\n          avg(value) as avg_value\n        FROM source_data\n        GROUP BY 1\n        ORDER BY 1\n</code></pre>"},{"location":"pipelines/#parallel-execution","title":"Parallel Execution","text":"<p>Stages at the same level execute in parallel:</p> <pre><code>stages:\n  # These run in parallel\n  - - name: sales_summary\n      query: \"SELECT region, sum(amount) FROM sales GROUP BY region\"\n\n    - name: user_summary  \n      query: \"SELECT country, count(*) FROM users GROUP BY country\"\n\n  # This runs after both above stages complete\n  - - name: combined_report\n      query: &gt;\n        SELECT s.region, s.total_sales, u.user_count\n        FROM sales_summary s\n        JOIN user_summary u ON s.region = u.country\n</code></pre>"},{"location":"pipelines/#debugging-stages","title":"Debugging Stages","text":"<p>Add debugging options to inspect your data:</p> <pre><code>stages:\n  - - name: debug_stage\n      query: \"SELECT * FROM source_data LIMIT 10\"\n      show: 10              # Print 10 rows to stdout\n      explain: true         # Show query execution plan\n      print_schema: true    # Print output schema\n</code></pre>"},{"location":"pipelines/#data-destinations","title":"Data Destinations","text":"<p>Destinations define where to write your pipeline results.</p>"},{"location":"pipelines/#file-destinations","title":"File Destinations","text":"<p>Write to various file formats:</p> <pre><code>destination:\n  type: file\n  name: output\n  location: ./results/output.parquet\n  format:\n    type: parquet\n    options: {}\n  single_file: true\n  partition_columns: [\"region\"]\n</code></pre>"},{"location":"pipelines/#delta-table-destinations","title":"Delta Table Destinations","text":"<p>Write to Delta Lake with advanced operations:</p> <pre><code>destination:\n  type: delta\n  name: target_table\n  location: s3://datalake/target/\n  write_mode:\n    operation: upsert    # append, replace, or upsert\n    params: [\"user_id\"]  # upsert key columns\n  partition_columns: [\"date\"]\n  schema:\n    - name: user_id\n      data_type: int64\n      nullable: false\n    - name: name\n      data_type: string\n      nullable: true\n</code></pre>"},{"location":"pipelines/#advanced-features","title":"Advanced Features","text":""},{"location":"pipelines/#parameter-templating","title":"Parameter Templating","text":"<p>Make pipelines reusable with parameter substitution:</p> <pre><code>sources:\n  - type: file\n    name: data\n    location: ./data/${environment}/${date}.csv\n\ndestination:\n  type: file\n  location: ./output/${environment}_${date}_results.parquet\n</code></pre> <p>Execute with parameters:</p> <pre><code>aqueducts run --file pipeline.yml --param environment=prod --param date=2024-01-15\n</code></pre>"},{"location":"pipelines/#schema-definitions","title":"Schema Definitions","text":"<p>Define explicit schemas for type safety:</p> <pre><code>sources:\n  - type: file\n    name: typed_data\n    format:\n      type: csv\n      options:\n        schema:\n          - name: user_id\n            data_type: int64\n            nullable: false\n          - name: score\n            data_type: \"decimal&lt;10,2&gt;\"\n            nullable: true\n</code></pre>"},{"location":"pipelines/#complex-data-types","title":"Complex Data Types","text":"<p>Aqueducts supports rich data types including:</p> <ul> <li>Basic types: <code>string</code>, <code>int64</code>, <code>float64</code>, <code>bool</code>, <code>date32</code></li> <li>Complex types: <code>list&lt;string&gt;</code>, <code>struct&lt;name:string,age:int32&gt;</code></li> <li>Temporal types: <code>timestamp&lt;millisecond,UTC&gt;</code>, <code>date32</code></li> <li>Decimal types: <code>decimal&lt;10,2&gt;</code> (precision, scale)</li> </ul> <p>See the Schema Reference for complete type documentation.</p>"},{"location":"pipelines/#examples","title":"Examples","text":"<p>Explore real-world pipeline patterns:</p> <ul> <li>Simple Pipeline: Basic CSV processing</li> <li>Complex Pipeline: Multi-source Delta operations</li> <li>ODBC Pipeline: Database integration</li> </ul>"},{"location":"pipelines/#best-practices","title":"Best Practices","text":"<p>Performance Tips</p> <ul> <li>Use partitioning for large datasets</li> <li>Leverage parallel stages for independent operations</li> <li>Consider Delta Lake for complex update patterns</li> <li>Use explicit schemas for better performance and type safety</li> </ul> <p>Common Pitfalls</p> <ul> <li>Ensure column names match between stages and joins</li> <li>Check data types when joining tables from different sources</li> <li>Be mindful of memory usage with large datasets</li> </ul>"},{"location":"schema_reference/","title":"Aqueduct","text":"<p>Definition for an \"Aqueduct\" data pipeline. An aqueduct defines a complete data processing pipeline with sources, transformation stages, and an optional destination. Most configuration uses sensible defaults to minimize verbosity.</p>"},{"location":"schema_reference/#root-properties","title":"Root Properties","text":""},{"location":"schema_reference/#version","title":"version","text":"<p>Property Details</p> <p>Type: <code>string</code> Required: No Default: <code>v2</code></p> <p>Schema version for migration compatibility</p>"},{"location":"schema_reference/#sources","title":"sources","text":"<p>Property Details</p> <p>Type: <code>array</code> Required: Yes</p> <p>Definition of the data sources for this pipeline</p>"},{"location":"schema_reference/#stages","title":"stages","text":"<p>Property Details</p> <p>Type: <code>array</code> Required: Yes</p> <p>A sequential list of transformations to execute within the context of this pipeline Nested stages are executed in parallel</p>"},{"location":"schema_reference/#destination","title":"destination","text":"<p>Property Details</p> <p>Type: <code>anyOf</code> Required: No</p> <p>Destination for the final step of the \"Aqueduct\" takes the last stage as input for the write operation</p> Option 1Option 2 <p>Type: <code>Destination</code></p> <p>Type: <code>null</code></p>"},{"location":"schema_reference/#type-definitions","title":"Type Definitions","text":""},{"location":"schema_reference/#source","title":"Source","text":"<p>A data source for aqueducts pipelines. Sources define where data is read from and include various formats and storage systems. Each source type has specific configuration options for its format and location.</p> An in-memory sourceA file sourceA directory sourceAn ODBC sourceA delta table source <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>name</code> <code>string</code> \u2713 Name of the in-memory table, existence will be checked at runtime - <p>Enum Values: - type: <code>in_memory</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>name</code> <code>string</code> \u2713 Name of the file source, will be the registered table name in the SQL context - <code>format</code> <code>SourceFileType</code> \u2713 File format of the file to be ingested Supports \"Parquet\" for parquet files, \"Csv\" for CSV files and \"Json\" for JSON files - <code>location</code> <code>Location</code> \u2713 A URL or Path to the location of the file Supports relative local paths - <code>storage_config</code> <code>object</code> \u2717 Storage configuration for the file Please reference the delta-rs github repo for more information on available keys (e.g. https://github.com/delta-io/delta-rs/blob/main/crates/aws/src/storage.rs) additionally also reference the \"object_store\" docs (e.g. https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html) <code>{}</code> <p>Enum Values: - type: <code>file</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>name</code> <code>string</code> \u2713 Name of the directory source, will be the registered table name in the SQL context - <code>format</code> <code>SourceFileType</code> \u2713 File format of the files to be ingested Supports \"Parquet\" for parquet files, \"Csv\" for CSV files and \"Json\" for JSON files - <code>partition_columns</code> <code>array</code> \u2717 Columns to partition the table by This is a list of key value tuples where the key is the column name and the value is a DataType <code>[]</code> <code>location</code> <code>Location</code> \u2713 A URL or Path to the location of the directory Supports relative local paths - <code>storage_config</code> <code>object</code> \u2717 Storage configuration for the directory Please reference the delta-rs github repo for more information on available keys (e.g. https://github.com/delta-io/delta-rs/blob/main/crates/aws/src/storage.rs) additionally also reference the \"object_store\" docs (e.g. https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html) <code>{}</code> <p>Enum Values: - type: <code>directory</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>name</code> <code>string</code> \u2713 Name of the ODBC source, will be the registered table name in the SQL context - <code>load_query</code> <code>string</code> \u2713 Query to execute when fetching data from the ODBC connection This query will execute eagerly before the data is processed by the pipeline Size of data returned from the query cannot exceed work memory - <code>connection_string</code> <code>string</code> \u2713 ODBC connection string Please reference the respective database connection string syntax (e.g. https://www.connectionstrings.com/postgresql-odbc-driver-psqlodbc/) - <p>Enum Values: - type: <code>odbc</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>name</code> <code>string</code> \u2713 Name of the delta source, will be the registered table name in the SQL context - <code>location</code> <code>Location</code> \u2713 A URL or Path to the location of the delta table Supports relative local paths - <code>storage_config</code> <code>object</code> \u2717 Storage configuration for the delta table Please reference the delta-rs github repo for more information on available keys (e.g. https://github.com/delta-io/delta-rs/blob/main/crates/aws/src/storage.rs) additionally also reference the \"object_store\" docs (e.g. https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html) <code>{}</code> <code>version</code> <code>integer | null</code> \u2717 Delta table version to read from When unspecified, will read the latest version - <code>timestamp</code> <code>string | null</code> \u2717 RFC3339 timestamp to read the delta table at When unspecified, will read the latest version - <p>Enum Values: - type: <code>delta</code></p>"},{"location":"schema_reference/#sourcefiletype","title":"SourceFileType","text":"<p>File type of the source file, supports \"Parquet\", \"Csv\" or \"Json\"</p> Parquet source optionsCsv source optionsJson source options <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>options</code> <code>ParquetSourceOptions</code> \u2713 - <p>Enum Values: - type: <code>parquet</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>options</code> <code>CsvSourceOptions</code> \u2713 - <p>Enum Values: - type: <code>csv</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>options</code> <code>JsonSourceOptions</code> \u2713 - <p>Enum Values: - type: <code>json</code></p>"},{"location":"schema_reference/#parquetsourceoptions","title":"ParquetSourceOptions","text":"<p>Properties:</p> Property Type Required Description Default <code>schema</code> <code>array</code> \u2717 schema to read this Parquet with Schema definition using universal Field types <code>[]</code>"},{"location":"schema_reference/#field","title":"Field","text":"<p>A field definition that can be used across all aqueducts backends. Fields define the structure of data with a name, data type, nullable flag, and optional description.</p> <p>Properties:</p> Property Type Required Description Default <code>name</code> <code>string</code> \u2713 The name of the field - <code>data_type</code> <code>string</code> \u2713 Data type specification. Examples: 'string', 'int64', 'bool', 'list', 'struct', 'timestamp', 'decimal&lt;10,2&gt;' - <code>nullable</code> <code>boolean</code> \u2717 Whether the field can contain null values <code>True</code> <code>description</code> <code>string | null</code> \u2717 Optional description of what this field represents -"},{"location":"schema_reference/#csvsourceoptions","title":"CsvSourceOptions","text":"<p>Properties:</p> Property Type Required Description Default <code>has_header</code> <code>boolean</code> \u2717 set to \"true\" to treat first row of CSV as the header column names will be inferred from the header, if there is no header the column names are \"column_1, column_2, ... column_x\" <code>True</code> <code>delimiter</code> <code>string</code> \u2717 set a delimiter character to read this CSV with <code>,</code> <code>schema</code> <code>array</code> \u2717 schema to read this CSV with Schema definition using universal Field types <code>[]</code>"},{"location":"schema_reference/#jsonsourceoptions","title":"JsonSourceOptions","text":"<p>Properties:</p> Property Type Required Description Default <code>schema</code> <code>array</code> \u2717 schema to read this JSON with Schema definition using universal Field types <code>[]</code>"},{"location":"schema_reference/#location","title":"Location","text":"<p>A file path or URL. File paths will be converted to file:// URLs. Examples: '/tmp/data.csv', './data.csv', 'https://example.com/data.csv', 's3://bucket/data.csv'</p>"},{"location":"schema_reference/#datatype","title":"DataType","text":"<p>Universal data type that can be converted to Arrow, Delta, and other formats. DataType supports all common data types and can be parsed from user-friendly string representations. This provides a unified schema definition that works across different backends. When used in YAML/JSON configurations, data types are specified as strings that are automatically parsed into the appropriate DataType variant.</p> <p>Data Type Format Examples</p> <p>Basic Types:</p> <ul> <li>\"string\" or \"utf8\" - UTF-8 string</li> <li>\"int32\", \"int\", or \"integer\" - 32-bit signed integer</li> <li>\"int64\" or \"long\" - 64-bit signed integer</li> <li>\"float32\" or \"float\" - 32-bit floating point</li> <li>\"float64\" or \"double\" - 64-bit floating point</li> <li>\"bool\" or \"boolean\" - Boolean value</li> <li>\"date32\" or \"date\" - Date as days since epoch</li> </ul> <p>Complex Types:</p> <ul> <li>\"list\" - List of strings <li>\"struct\" - Struct with name and age fields <li>\"decimal&lt;10,2&gt;\" - Decimal with precision 10, scale 2</li> <li>\"timestamp\" - Timestamp with time unit and timezone <li>\"map\" - Map from string keys to int32 values <p>YAML Configuration Example:</p> <pre><code>schema:\n  - name: user_id\n    data_type: int64\n    nullable: false\n  - name: email\n    data_type: string\n    nullable: true\n  - name: scores\n    data_type: \"list&lt;float64&gt;\"\n    nullable: true\n  - name: profile\n    data_type: \"struct&lt;name:string,age:int32&gt;\"\n    nullable: true\n</code></pre> Basic TypesOption 2Option 3Option 4Option 5Option 6Option 7Option 8Option 9Option 10Option 11Option 12Option 13Option 14Option 15Option 16 <p>Type: <code>string</code></p> <p>Allowed Values: - <code>Boolean</code> - <code>Int8</code> - <code>Int16</code> - <code>Int32</code> - <code>Int64</code> - <code>UInt8</code> - <code>UInt16</code> - <code>UInt32</code> - <code>UInt64</code> - <code>Float32</code> - <code>Float64</code> - <code>Utf8</code> - <code>LargeUtf8</code> - <code>Binary</code> - <code>LargeBinary</code> - <code>Date32</code> - <code>Date64</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>FixedSizeBinary</code> <code>integer</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Time32</code> <code>TimeUnit</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Time64</code> <code>TimeUnit</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Timestamp</code> <code>array</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Duration</code> <code>TimeUnit</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Interval</code> <code>IntervalUnit</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Decimal128</code> <code>array</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Decimal256</code> <code>array</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>List</code> <code>Field</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>LargeList</code> <code>Field</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>FixedSizeList</code> <code>array</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Struct</code> <code>array</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Map</code> <code>array</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Union</code> <code>array</code> \u2713 - <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>Dictionary</code> <code>array</code> \u2713 -"},{"location":"schema_reference/#stage","title":"Stage","text":"<p>A processing stage in an aqueducts pipeline. Stages execute SQL queries against the available data sources and previous stage results. Each stage creates a named table that can be referenced by subsequent stages.</p> <p>Properties:</p> Property Type Required Description Default <code>name</code> <code>string</code> \u2713 Name of the stage, used as the table name for the result of this stage - <code>query</code> <code>string</code> \u2713 SQL query that is executed against a datafusion context. Check the datafusion SQL reference for more information https://datafusion.apache.org/user-guide/sql/index.html - <code>show</code> <code>integer | null</code> \u2717 When set to a value of up to \"usize\", will print the result of this stage to the stdout limited by the number Set value to 0 for unlimited output (capped at 500 rows to avoid terminal overflow) - <code>explain</code> <code>boolean</code> \u2717 When set to 'true' the stage will output the query execution plan <code>False</code> <code>explain_analyze</code> <code>boolean</code> \u2717 When set to 'true' the stage will output the query execution plan with added execution metrics <code>False</code> <code>print_schema</code> <code>boolean</code> \u2717 When set to 'true' the stage will pretty print the output schema of the executed query <code>False</code>"},{"location":"schema_reference/#destination_1","title":"Destination","text":"<p>Target output destination for aqueducts pipelines. Destinations define where processed data is written and include various formats and storage systems with their specific configuration options.</p> An in-memory destinationA file output destinationAn ODBC insert query to write to a DB tableA delta table destination <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>name</code> <code>string</code> \u2713 Name to register the table with in the provided \"SessionContext\" - <p>Enum Values: - type: <code>in_memory</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>name</code> <code>string</code> \u2713 Name of the file to write - <code>location</code> <code>Location</code> \u2713 Location of the file as a URL e.g. file:///tmp/output.csv, s3://bucket_name/prefix/output.parquet, s3:://bucket_name/prefix - <code>format</code> <code>DestinationFileType</code> \u2713 File format, supported types are Parquet and CSV - <code>single_file</code> <code>boolean</code> \u2717 Describes whether to write a single file (can be used to overwrite destination file) <code>True</code> <code>partition_columns</code> <code>array</code> \u2717 Columns to partition table by <code>[]</code> <code>storage_config</code> <code>object</code> \u2717 Object store storage configuration <code>{}</code> <p>Enum Values: - type: <code>file</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>name</code> <code>string</code> \u2713 Name of the destination - <code>connection_string</code> <code>string</code> \u2713 ODBC connection string Please reference the respective database connection string syntax (e.g. https://www.connectionstrings.com/postgresql-odbc-driver-psqlodbc/) - <code>write_mode</code> <code>WriteMode</code> \u2713 Strategy for performing ODBC write operation - <code>batch_size</code> <code>integer</code> \u2717 Batch size for inserts (defaults to 1000) <code>1000</code> <p>Enum Values: - type: <code>odbc</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>name</code> <code>string</code> \u2713 Name of the destination - <code>location</code> <code>Location</code> \u2713 A URL or Path to the location of the delta table Supports relative local paths - <code>write_mode</code> <code>DeltaWriteMode</code> \u2713 Write mode for the delta destination - <code>storage_config</code> <code>object</code> \u2717 Storage configuration for the delta table Please reference the delta-rs github repo for more information on available keys (e.g. https://github.com/delta-io/delta-rs/blob/main/crates/aws/src/storage.rs) additionally also reference the \"object_store\" docs (e.g. https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html) <code>{}</code> <code>partition_columns</code> <code>array</code> \u2717 Partition columns for the delta table <code>[]</code> <code>table_properties</code> <code>object</code> \u2717 DeltaTable table properties: https://docs.delta.io/latest/table-properties.html <code>{}</code> <code>metadata</code> <code>object</code> \u2717 Custom metadata to include with the table creation <code>{}</code> <code>schema</code> <code>array</code> \u2717 Table schema definition using universal Field types <code>[]</code> <p>Enum Values: - type: <code>delta</code></p>"},{"location":"schema_reference/#destinationfiletype","title":"DestinationFileType","text":"<p>File type and options for destinations</p> Parquet options map, please refer to TableParquetOptions for possible optionsCSV optionsJson destination, no supported options <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>options</code> <code>object</code> \u2713 - <p>Enum Values: - type: <code>parquet</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <code>options</code> <code>CsvDestinationOptions</code> \u2713 - <p>Enum Values: - type: <code>csv</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>type</code> <code>string</code> \u2713 - <p>Enum Values: - type: <code>json</code></p>"},{"location":"schema_reference/#csvdestinationoptions","title":"CsvDestinationOptions","text":"<p>CSV destination options</p> <p>Properties:</p> Property Type Required Description Default <code>has_header</code> <code>boolean</code> \u2717 Set to \"true\" to include headers in CSV <code>True</code> <code>delimiter</code> <code>string</code> \u2717 Set delimiter character to write CSV with <code>,</code> <code>compression</code> <code>string | null</code> \u2717 Compression type for CSV output -"},{"location":"schema_reference/#writemode","title":"WriteMode","text":"<p>Write modes for the \"Destination\" output.</p> \"Append\": appends data to the \"Destination\"\"Custom\": Inserts data with a prepared statement. Option to perform any number of (non-insert) preliminary statements <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>operation</code> <code>string</code> \u2713 - <p>Enum Values: - operation: <code>append</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>operation</code> <code>string</code> \u2713 - <code>transaction</code> <code>CustomStatements</code> \u2713 - <p>Enum Values: - operation: <code>custom</code></p>"},{"location":"schema_reference/#customstatements","title":"CustomStatements","text":"<p>SQL statements for \"Custom\" write mode.</p> <p>Properties:</p> Property Type Required Description Default <code>pre_insert</code> <code>string | null</code> \u2717 Optional (non-insert) preliminary statement - <code>insert</code> <code>string</code> \u2713 Insert prepared statement -"},{"location":"schema_reference/#deltawritemode","title":"DeltaWriteMode","text":"<p>Write mode for delta destinations</p> Append data to the destination tableUpsert data using the specified merge columns for uniquenessReplace data matching the specified conditions <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>operation</code> <code>string</code> \u2713 - <p>Enum Values: - operation: <code>append</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>operation</code> <code>string</code> \u2713 - <code>params</code> <code>array</code> \u2713 - <p>Enum Values: - operation: <code>upsert</code></p> <p>Type: <code>object</code></p> <p>Properties:</p> Property Type Required Description Default <code>operation</code> <code>string</code> \u2713 - <code>params</code> <code>array</code> \u2713 - <p>Enum Values: - operation: <code>replace</code></p>"},{"location":"schema_reference/#replacecondition","title":"ReplaceCondition","text":"<p>Condition used to build a predicate for data replacement.</p> <p>Properties:</p> Property Type Required Description Default <code>column</code> <code>string</code> \u2713 Column name to match against - <code>value</code> <code>string</code> \u2713 Value to match for replacement -"}]}